# -*- coding: utf-8 -*-
"""LinkedIn_AIML (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SvvCZm_1x4vt4jNtxWZ3bq6Sh6H2TuAy

## Importing Required Libraries and Loading the Data
"""


import json
import pandas as pd
import re
import string
import os
from datetime import datetime, timezone
from rag_remark_generator import generate_ai_remark
import numpy as np

# from IPython.print import print
# file_path = r'C:\Users\Dell\Desktop\linkedin-scrape-data\data\apify_jobs_raw.json'  # <-- "r" handles backslashes'  # Use relative path for Node.js compatibility

base_dir = os.path.dirname(__file__)
file_path = os.path.join(base_dir, '..', 'data', 'filtered.json')

#Load JSON data
with open(file_path, 'r', encoding='utf-8') as f:
    data = json.load(f)

# Normalize JSON depending on structure
if isinstance(data, dict) and 'data' in data:
    jobs = data['data']
elif isinstance(data, list):
    jobs = data
else:
    raise ValueError("Unrecognized JSON structure.")

# Convert to DataFrame
df = pd.json_normalize(jobs)


# Preview
print(f" Remote jobs found: {df.shape[0]}")
# print(df[['id', 'title', 'company.name', 'location.linkedinText', 'workRemoteAllowed']].head())

"""## KPI 1: Job Description Quality KPI"""

def jd_quality_score(text):
    text = str(text)
    score = 0.0
    lower_text = text.lower()
    length = len(text)

    if any(kw in lower_text for kw in ['responsibilities', 'requirements', 'qualifications', 'skills']):
        score += 0.4

    if '-' in text or '•' in text:
        score += 0.3

    symbol_count = sum(1 for char in text if char in string.punctuation)
    symbol_ratio = symbol_count / max(length, 1)
    if symbol_ratio < 0.05:
        score += 0.3

    return round(min(score, 1.0), 2)

df['kpi_jd_quality'] = df['descriptionText'].apply(jd_quality_score)
# print(df[['title', 'kpi_jd_quality']].head()

"""## KPI 2: Domain Fit KPI (QA, Test Automation, Web Dev, AI, ML, UI/UX)

"""

target_domains = {
    "qa": [
        "qa", "quality assurance", "quality control", "test automation", "manual testing", "testing",
        "test engineer", "automated testing", "test case", "test plan", "test execution", "bug report",
        "defect", "regression testing", "functional testing", "performance testing", "load testing",
        "stress testing", "security testing", "penetration testing", "integration testing", "unit testing",
        "uat", "user acceptance testing", "test coverage", "cypress", "selenium", "jmeter", "postman",
        "test rail", "bug tracking", "jira", "smoke testing", "agile testing", "scrum testing",
        "test script", "ci/cd testing", "exploratory testing", "test data", "test automation framework"
    ],
    "development": [
        "developer", "development", "software engineer", "software developer", "full stack", "fullstack",
        "backend", "back end", "frontend", "front end", "web developer", "mobile developer",
        "app developer", "web development", "mobile development", "server-side", "client-side",
        "build", "implement", "refactor", "codebase", "clean code", "api", "rest api", "graphql",
        "microservices", "architecture", "mvc", "oop", "design patterns", "tdd", "bdd",
        "version control", "git", "github", "gitlab", "bitbucket", "pull request", "code review",
        "dev work", "scrum", "agile", "sprint", "jira", "feature development", "bug fixing",
        "debugging", "deployment", "release", "unit tests", "code optimization", "ci/cd",
        "web", "website", "app", "application", "system design", "backend systems", "frontend ui",
        "tech stack", "technology stack", "api integration", "authentication", "authorization",
        "websocket", "sdk", "library", "framework", "npm", "yarn", "html", "css", "javascript",
        "typescript", "react", "vue", "angular", "svelte", "jquery", "bootstrap", "tailwind",
        "node", "express", "next.js", "nuxt.js", "vite", "php", "laravel", "symfony",
        "ruby on rails", "java", "spring", "spring boot", "c#", ".net", "asp.net", "flask", "django",
        "fastapi", "golang", "go", "kotlin", "swift", "objective-c", "android", "ios", "react native",
        "flutter", "xamarin", "cordova", "electron", "desktop app", "cross-platform", "mongodb",
        "postgresql", "mysql", "sqlite", "nosql", "redis", "firebase", "supabase", "graphql", "rest"
    ],
    "ai": [
        "ai", "artificial intelligence", "machine learning", "ml", "deep learning", "generative ai",
        "genai", "large language model", "llm", "foundation model", "chatgpt", "openai",
        "pytorch", "tensorflow", "keras", "scikit-learn", "huggingface", "vector store",
        "vector database", "pinecone", "weaviate", "chroma", "langchain", "retrieval-augmented generation",
        "rag", "prompt engineering", "prompt tuning", "fine-tuning", "inference", "model deployment",
        "mlops", "ml pipeline", "feature engineering", "model training", "model evaluation",
        "data science", "data scientist", "statistical modeling", "predictive modeling", "nlp",
        "natural language processing", "text classification", "entity recognition", "computer vision",
        "image recognition", "object detection", "cv", "speech recognition", "voice ai", "recommendation system",
        "data mining", "classification", "regression", "clustering", "unsupervised learning",
        "reinforcement learning", "transfer learning", "bayesian", "neural network", "transformer",
        "attention mechanism", "tokenization", "embedding", "zero-shot", "few-shot", "ai agent"
    ],
    "uiux": [
        "ui", "ux", "ui/ux", "user interface", "user experience", "product designer",
        "ux designer", "ui designer", "interaction designer", "visual designer",
        "experience designer", "user researcher", "ux researcher", "usability expert",
        "ux strategist", "design researcher", "human-centered design", "ux/ui",
        "figma", "sketch", "adobe xd", "invision", "zeplin", "wireframe", "prototyping",
        "mockup", "design system", "design tokens", "user journey", "user flow",
        "persona", "usability", "accessibility", "a/b testing", "interface design",
        "information architecture", "responsive design", "mobile-first", "adaptive design",
        "design thinking", "design sprint", "design principles", "hci", "heuristic evaluation",
        "conversion rate", "ux metrics", "user testing", "qualitative research",
        "quantitative research", "heatmaps", "surveys", "storyboarding", "lo-fi design",
        "hi-fi prototype", "experience map", "customer experience", "cx"
    ],
    "cloud_devops": [
        "cloud", "aws", "amazon web services", "azure", "gcp", "google cloud", "cloud engineer",
        "infrastructure", "cloud infrastructure", "infrastructure as code", "serverless",
        "cloud-native", "platform engineer", "cloud computing", "vpc", "load balancer", "dns",
        "auto scaling", "cloudformation", "bicep", "lambda", "cloud deployment", "availability zone",
        "multi-region", "s3", "ec2", "rds", "eks", "aks", "gke", "iam", "identity management"
    ],
    "devops": [
        "devops", "devops engineer", "ci/cd", "jenkins", "circleci", "github actions",
        "gitlab ci", "pipeline", "build pipeline", "release pipeline", "automation", "build automation",
        "infrastructure as code", "terraform", "ansible", "puppet", "chef", "helm", "docker",
        "kubernetes", "container orchestration", "containers", "monitoring", "observability", "alerting",
        "prometheus", "grafana", "splunk", "datadog", "logz", "logging", "deployment", "release",
        "scripting", "bash scripting", "shell scripting", "yaml", "configuration management", "rollback",
        "blue-green deployment", "canary release", "incident management", "runbook", "devops pipeline",
        "env management", "infrastructure monitoring", "uptime", "downtime", "site reliability",
        "sre", "platform reliability", "build tools", "artifact", "nexus", "artifactory"
    ],
    "data_engineering": [
        "data engineer", "etl", "data pipeline", "data ingestion", "batch processing",
        "stream processing", "streaming data", "airflow", "dagster", "luigi", "kafka",
        "spark", "hadoop", "flink", "big data", "data warehouse", "data lake", "snowflake",
        "redshift", "databricks", "bigquery", "data modeling", "star schema", "dbt", "sql",
        "nosql", "mongodb", "postgresql", "mysql", "data platform", "data infrastructure",
        "data architecture", "schema design", "parquet", "avro", "orjson", "json", "csv",
        "data transformation", "data validation", "data quality", "data catalog", "metadata",
        "pipeline orchestration"
    ],
    "cybersecurity": [
        "cybersecurity", "security engineer", "security analyst", "security architect",
        "application security", "network security", "information security", "infosec",
        "penetration testing", "vulnerability assessment", "ethical hacking", "threat modeling",
        "risk assessment", "incident response", "security operations", "soc", "red team",
        "blue team", "devsecops", "zero trust", "iam", "identity and access management",
        "authentication", "authorization", "sso", "mfa", "owasp", "nmap", "burp suite",
        "wireshark", "siem", "firewall", "encryption", "ssl", "tls", "certificate management",
        "compliance", "gdpr", "iso 27001", "hipaa"
    ]
}


# Flatten keyword list for matching
all_domain_keywords = set(kw.lower() for keywords in target_domains.values() for kw in keywords)

# Scoring function
def domain_fit_score(row):
    combined_text = f"{row.get('title', '')} {row.get('descriptionText', '')} {' '.join(row.get('company.industries', []))} {' '.join(row.get('company.specialities', []))}".lower()
    matched = sum(1 for kw in all_domain_keywords if kw in combined_text)
    return round(min(matched / 5.0, 1.0), 2)

# Apply
df['kpi_domain_fit'] = df.apply(domain_fit_score, axis=1)

# Show output
# print(df[['title', 'kpi_domain_fit']].head())

"""## KPI 3: Seniority Alignment KPI

"""

# Inferred from descriptionText

seniority_high_keywords = ['mid-senior', 'senior', 'lead', 'director', 'executive']
seniority_low_keywords = ['internship', 'intern', 'entry level', 'junior']

def seniority_alignment_from_description(text):
    text = str(text).lower()

    if any(keyword in text for keyword in seniority_low_keywords):
        return 0.2
    elif any(keyword in text for keyword in seniority_high_keywords):
        return 1.0
    else:
        return 0.5

df['kpi_seniority_alignment'] = df['descriptionText'].apply(seniority_alignment_from_description)

# print results
# print(df[['title', 'kpi_seniority_alignment']].head())

"""## KPI 4: Location Priority KPI"""
us_canada = {'united states', 'usa', 'canada'}
europe = {'united kingdom', 'uk', 'germany', 'netherlands', 'france', 'denmark', 'sweden', 'norway', 'finland', 'switzerland'}
middle_east = {'uae', 'united arab emirates', 'saudi arabia', 'qatar', 'oman', 'kuwait', 'bahrain'}
discard = {'pakistan', 'india', 'bangladesh', 'nepal', 'sri lanka', 'china', 'indonesia', 'vietnam', 'malaysia', 'thailand', 'israel'}

def location_priority_score(locations):
    if isinstance(locations, list):
        for loc in locations:
            country = str(loc.get('parsed', {}).get('country', '')).strip().lower()

            if country in discard:
                return 0.0
            elif country in us_canada:
                return 1.0
            elif country in europe:
                return 0.8
            elif country in middle_east:
                return 0.6
            elif country:
                return 0.5
    return 0.5

df['kpi_location_priority'] = df['company.locations'].apply(location_priority_score)
# print(df[['title', 'company.locations', 'kpi_location_priority']].head())


'''## KPI 5: Company Specialties Match KPI'''

target_specialties = [
    # QA & Testing
    "qa", "quality assurance", "test automation", "testing", "manual testing",
    "selenium", "cypress", "playwright", "jmeter", "postman", "api testing",
    "performance testing", "security testing", "regression testing", "functional testing",
    "unit testing", "integration testing", "test case", "test plan", "jira", "testrail",
    "agile testing", "bdd", "tdd", "cucumber", "qa engineer", "software tester",

    # Web Development (Frontend & Backend)
    "frontend", "backend", "web development", "web developer", "full stack",
    "javascript", "typescript", "react", "angular", "vue.js", "html", "css",
    "scss", "less", "bootstrap", "tailwind css", "webpack", "babel", "npm", "yarn",
    "redux", "next.js", "nuxt.js", "svelte", "dom", "node.js", "express.js",
    "python", "django", "flask", "java", "spring boot", "c#", ".net", "php",
    "laravel", "ruby", "rails", "go", "golang", "rest api", "graphql",
    "microservices", "api development", "web engineer",

    # AI, Machine Learning & Data Science
    "ai", "machine learning", "ml", "deep learning", "data science", "nlp",
    "artificial intelligence", "computer vision", "reinforcement learning",
    "data analysis", "data engineering", "mlops", "generative ai", "llm",
    "large language model", "prompt engineering", "statistical modeling",
    "predictive analytics", "r", "pytorch", "tensorflow", "keras", "scikit-learn",
    "pandas", "numpy", "scipy", "data visualization", "tableau", "power bi",
    "big data", "hadoop", "spark", "data scientist", "ml engineer", "ai engineer",

    # UI/UX Design
    "ui", "ux", "ui/ux", "user interface", "user experience", "design", "figma",
    "user research", "usability testing", "wireframing", "prototyping", "design systems",
    "information architecture", "interaction design", "visual design", "responsive design",
    "accessibility", "adobe xd", "sketch", "invision", "user flows", "mockups",
    "design thinking", "human-computer interaction", "product design", "service design",

    # Software Engineering, DevOps, Cloud & Infrastructure
    "software engineering", "devops", "cloud", "infrastructure",
    "software development", "agile", "scrum", "kanban", "git", "version control",
    "ci/cd", "continuous integration", "continuous delivery", "containerization",
    "docker", "kubernetes", "aws", "azure", "gcp", "cloud computing",
    "system design", "architecture", "linux", "unix", "scripting", "bash", "shell",
    "automation", "security", "cybersecurity", "networking", "virtualization",
    "ansible", "terraform", "jenkins", "gitlab ci", "github actions", "sre",
    "site reliability engineering", "database", "sql", "nosql", "mongodb",
    "postgresql", "mysql", "redis", "database management", "system administration",
    "backend engineer", "network engineer", "solutions architect"
]

def company_specialties_score(specialties):
    if not isinstance(specialties, list):
        return 0.0
    
    matches = [spec for spec in specialties if spec.lower().strip() in target_specialties]
    count = len(matches)

    if count >= 3:
        return 1.0
    elif count == 2:
        return 0.66
    elif count == 1:
        return 0.33
    else:
        return 0.0

df['kpi_company_specialties'] = df['company.specialities'].apply(company_specialties_score)

# Preview the result
# print(df[['title', 'company.specialities', 'kpi_company_specialties']].head())


"""## KPI 6: Salary Attractiveness KPI"""

def salary_score(row):
    salary_str = row.get('salary', '')
    if not isinstance(salary_str, str) or salary_str.strip() == '':
        return 0.5  # Missing or invalid salary string

    # Remove commas and extract numbers
    salary_str = salary_str.replace(',', '')
    matches = re.findall(r'\d+', salary_str)

    if not matches:
        return 0.5

    # Convert to float and calculate average
    try:
        salary_values = list(map(float, matches))
        if len(salary_values) == 1:
            avg_salary = salary_values[0]
        else:
            avg_salary = sum(salary_values) / len(salary_values)
    except:
        return 0.5

    # Salary scoring thresholds
    if avg_salary >= 200000:
        return 1.0
    elif avg_salary >= 150000:
        return 0.8
    elif avg_salary >= 100000:
        return 0.6
    elif avg_salary >= 60000:
        return 0.4
    else:
        return 0.2

df['kpi_salary'] = df.apply(salary_score, axis=1)
# print(df[['title', 'salary', 'kpi_salary']].head())


"""## KPI 7: Company Size KPI"""

def company_size_score(emp_count):
    try:
        count = int(emp_count)
    except:
        return 0.4  # default score if invalid or missing

    if count == 1:
        return 1.0
    elif 2 <= count <= 10:
        return 0.9
    elif 11 <= count <= 50:
        return 0.8
    elif 51 <= count <= 200:
        return 0.7
    elif 201 <= count <= 500:
        return 0.6
    elif 501 <= count <= 1000:
        return 0.5
    elif 1001 <= count <= 5000:
        return 0.4
    elif 5001 <= count <= 10000:
        return 0.3
    else:
        return 0.2

df['kpi_company_size'] = df['company.employeeCount'].apply(company_size_score)
# print(df[['title', 'company.employeeCount', 'kpi_company_size']].head())

"""## KPI 8: Company Popularity KPI"""

def company_popularity_score(followers):
    try:
        followers = int(followers)
    except:
        return 0.4  # default if missing

    if followers > 500000:
        return 1.0
    elif followers > 100000:
        return 0.9
    elif followers > 50000:
        return 0.8
    elif followers > 10000:
        return 0.7
    elif followers > 1000:
        return 0.5
    else:
        return 0.3

df['kpi_company_popularity'] = df['company.followerCount'].apply(company_popularity_score)
# print(df[['title', 'company.followerCount', 'kpi_company_popularity']].head())

"""## KPI 9: Company Industry Match KPI"""

preferred_industries = [
    "Information Technology", "Computer Software", "AI", "Internet", "Software Development",
    "Staffing and Recruiting", "Web Development", "Machine Learning", "IT Services",
    "Fintech", "Edtech", "Healthtech", "Crypto", "Logistics", "Media",
    "E-commerce", "Ride hailing"
]

def industry_match_score(industry_list):
    if not isinstance(industry_list, list):
        return 0.4  # fallback score

    matches = sum(1 for industry in industry_list if any(pref.lower() in industry.lower() for pref in preferred_industries))
    return round(min(matches / 3.0, 1.0), 2)

df['kpi_industry_match'] = df['company.industries'].apply(industry_match_score)
# print(df[['title', 'company.industries', 'kpi_industry_match']].head())


"""## KPI 10: Job Popularity KPI"""

def job_popularity_score(views):
    try:
        views = int(views)
    except:
        return 0.4

    if views >= 10000:
        return 0.1
    elif views >= 5000:
        return 0.2
    elif views >= 1000:
        return 0.4
    elif views >= 300:
        return 0.6
    elif views >= 100:
        return 0.8
    elif views >= 50:
        return 1.0
    else:
        return 0.5

df['kpi_job_popularity'] = df['views'].apply(job_popularity_score)
# print(df[['title', 'views', 'kpi_job_popularity']].head())

"""## KPI 11: Job Freshness KPI"""

def job_freshness_score(posted_date):
    try:
        posted = datetime.fromisoformat(posted_date.replace("Z", ""))
        days_old = (datetime.now(timezone.utc) - posted).days
    except:
        return 0.1

    if days_old <= 0:
        return 1.0
    elif days_old <= 2:
        return 0.8
    elif days_old <= 6:
        return 0.6
    elif days_old <= 14:
        return 0.4
    elif days_old <= 30:
        return 0.2
    else:
        return 0.1

df['kpi_job_freshness'] = df['postedDate'].apply(job_freshness_score)
# print(df[['title', 'postedDate', 'kpi_job_freshness']].head())

"""## KPI 12: Employment Type KPI"""

preferred_types = ['full_time', 'contract']

def employment_type_score(emp_type):
    if isinstance(emp_type, str):
        emp_type = emp_type.lower()
        if emp_type in preferred_types:
            return 1.0
        else:
            return 0.5
    return 0.4

df['kpi_employment_type'] = df['employmentType'].apply(employment_type_score)
# print(df[['title', 'employmentType', 'kpi_employment_type']].head())

"""## KPI 13: Contact Info Present"""

def contact_info_score(text):
    if pd.isna(text):
        return 0.0

    text = str(text)
    score = 0.0

    email_pattern = r'[\w\.-]+@[\w\.-]+'
    phone_pattern = r'\+?\d[\d\-\s]{8,}'
    url_pattern = r'https?://[^\s)]+'

    if re.search(email_pattern, text):
        score += 0.33
    if re.search(phone_pattern, text):
        score += 0.33
    if re.search(url_pattern, text):
        score += 0.33

    return round(min(score, 1.0), 2)

df['kpi_contact_info'] = df['descriptionText'].apply(contact_info_score)
# print(df[['title', 'kpi_contact_info']].head())

"""## KPI 14: Skills Explicitness KPI"""

skills_keywords = [
    # QA & Testing Skills
    "qa", "quality assurance", "automation", "test automation", "selenium", "cypress",
    "playwright", "jmeter", "postman", "test case", "test plan", "bug tracking", "jira",
    "api testing", "performance testing", "security testing", "regression testing",
    "functional testing", "unit testing", "integration testing", "bdd", "tdd", "cucumber",
    "pytest", "junit", "testng", "software testing",

    # AI/Machine Learning/Data Science Skills
    "machine learning", "deep learning", "nlp", "ai", "ml", "pytorch", "tensorflow", "keras",
    "scikit-learn", "pandas", "numpy", "data analysis", "data modeling", "statistical analysis",
    "computer vision", "reinforcement learning", "generative ai", "llms", "prompt engineering",
    "data visualization", "matplotlib", "seaborn", "plotly", "spark", "hadoop", "etl", "data cleansing",

    # Web Development (Frontend & Backend) Skills
    "react", "vue", "angular", "javascript", "typescript", "html", "css", "scss", "less",
    "node", "express", "flask", "django", "spring boot", "c#", ".net", "php", "laravel",
    "ruby on rails", "go", "gin", "fastapi", "restful apis", "graphql", "microservices",
    "webpack", "babel", "redux", "next.js", "nuxt.js", "svelte", "bootstrap", "tailwind css",
    "web components", "responsive design", "server-side rendering",

    # UI/UX/Design Skills
    "figma", "adobe xd", "sketch", "wireframe", "prototype", "user research", "usability testing",
    "design systems", "information architecture", "interaction design", "visual design",
    "user flows", "mockups", "design thinking", "personas", "storyboarding", "user-centered design",

    # General Software Development, DevOps, Cloud & Database Skills
    "git", "version control", "docker", "kubernetes", "aws", "azure", "gcp", "cloud computing",
    "ci/cd", "continuous integration", "continuous delivery", "jenkins", "gitlab ci", "github actions",
    "linux", "shell scripting", "bash", "python scripting", "api design", "system design",
    "data structures", "algorithms", "problem-solving", "agile methodologies", "scrum", "kanban",
    "sql", "postgresql", "mysql", "mongodb", "redis", "database design", "database management",
    "debugging", "troubleshooting", "software architecture", "clean code", "code review"
]

def skills_explicitness(text):
    if pd.isna(text):
        return 0.0
    text = text.lower()
    match_count = sum(1 for skill in skills_keywords if skill in text)
    return round(min(match_count / 5.0, 1.0), 2)

df['kpi_skills_explicitness'] = df['descriptionText'].apply(skills_explicitness)
# print(df[['title', 'kpi_skills_explicitness']].head())

"""## KPI 15: Experience Threshold KPI"""

experience_keywords = [
    '3+ years', '4+ years', '5+ years', '6+ years', '7+ years',
    '8+ years', 'experience with', 'mid-senior', 'senior'
]

# Patterns to identify seniority
seniority_high = ['mid-senior level', 'senior', 'director', 'executive']
seniority_low = ['internship', 'entry level', 'junior']

# Function to extract and score experience/seniority from descriptionText
def experience_score_from_description(desc):
    desc = desc.lower().strip() if pd.notna(desc) else ''

    # Look for seniority signals in description text
    if re.search(r'\bintern(ship)?\b', desc):
        return 0.0
    elif re.search(r'\b(entry level|junior)\b', desc):
        return 0.2
    elif any(kw in desc for kw in experience_keywords):
        return 1.0
    else:
        return 0.5

# Apply the function
df['kpi_experience_threshold'] = df['descriptionText'].apply(experience_score_from_description)

# --- Predict Domain for Each Job and Append Column --- 

def predict_domain(title, description, keyword_dict):
    title_lower = title.lower()
    combined_text = f"{title} {description}".lower()

    # Step 1: If any domain keyword appears in title, assign domain immediately
    for domain, keywords in keyword_dict.items():
        for kw in keywords:
            if kw in title_lower:
                return domain

    # Step 2: Score based on matches in combined text (fallback if title didn't help)
    domain_scores = {domain: 0 for domain in keyword_dict}
    for domain, keywords in keyword_dict.items():
        for kw in keywords:
            if kw in combined_text:
                domain_scores[domain] += 1

    best_domain = max(domain_scores, key=domain_scores.get)
    return best_domain if domain_scores[best_domain] > 0 else "other"


# Apply to DataFrame
df['predicted_domain'] = df.apply(
    lambda row: predict_domain(
        str(row.get('title', '')),
        str(row.get('descriptionText', '')),
        target_domains
    ),
    axis=1
)



# print results
# print(df[['title', 'descriptionText', 'kpi_experience_threshold']].head(5))

"""## Final Step: Score Aggregation and Tier Assignment
We now calculate the weighted score and classify each job as:
- **Green** (≥ 0.80)
- **Yellow** (0.60–0.79)
- **Red** (< 0.60)

"""

kpi_columns = [col for col in df.columns if col.startswith('kpi_')]

# Calculate the final score as mean of all KPIs
df['final_score'] = df[kpi_columns].mean(axis=1).round(2)

# Assign tiers based on final_score
def assign_tier(score):
    if score >= 0.70:
        return "Green"
    elif score >= 0.50:
        return "Yellow"
    else:
        return "Red"

df['tier'] = df['final_score'].apply(assign_tier)

# print final results
print(df[['title', 'final_score', 'tier'] + kpi_columns].head(100))

# Replace NaN with None (null in JSON)
df = df.where(pd.notnull(df), None)

# Convert to list of dicts
job_list = df.to_dict(orient='records')

# --- Generate AI Remarks ---
print(">> Generating AI Remarks using RAG...")
updated_jobs = generate_ai_remark(job_list)
print(">> AI Remarks generation completed.")

# Save final output
with open("data/scored_jobs_output.json", "w", encoding="utf-8") as f:
    json.dump(updated_jobs, f, ensure_ascii=False, indent=2)

print("Final output with AI remarks saved to 'scored_jobs_output.json'")
